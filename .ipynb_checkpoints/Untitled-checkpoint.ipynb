{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('df_training_scholarjet.csv') as csv_file:\n",
    "    my_reader = csv.reader(csv_file)\n",
    "    count = 0\n",
    "    X = []\n",
    "    revenue_labels = []\n",
    "    conversion_labels = []\n",
    "    float_converter = {'Onboarding': '0.0', 'Retention': '1.0', 'Unmanaged': '2.0', 'Enrolled': '0.0', 'Active': '1.0', 'In Progress': '2.0', 'Inactive': '3.0', 'Unconfirmed': '4.0', 'Business': '0.0', 'Trade': '1.0', 'US': '0.0', 'CA': '1.0', 'Internal Application': '0.0', 'Search - Paid': '1.0', 'Internal Customer Scrape': '2.0', 'Affiliates': '3.0', 'External Application': '4.0', 'Social - Paid': '5.0', 'Other': '6.0', 'Email': '7.0', 'Display - Retargeting': '8.0', 'Gateway': '9.0', 'Display - Acquisition': '10.0', 'Bulk Upload': '11.0', 'Quotes': '12.0', 'Referral': '13.0', 'Self ID': '14.0', 'Partners': '15.0', 'None': '0.0', 'Primary': '1.0', 'Purchaser': '2.0', 'Other': '3.0', '6to10': '2.0', '11to50': '3.0', '50plus': '4.0', '2to5': '5.0', '1to2': '1.0', '3to5': '3.0', '11to25': '4.0', '25plus': '5.0', 'lessthan1': '1.0', '1to5': '2.0', '5to25': '3.0', '100plus': '4.0', '25to100': '5.0', 'directEIN': '0.0', 'phone': '1.0', 'email': '2.0', 'liveTransfer': '3.0', 'other': '4.0', 'directOther': '5.0'}\n",
    "    for row in my_reader:\n",
    "        if count > 0:\n",
    "            new_row = []\n",
    "            for item in row[4:]:\n",
    "                if len(item) == 0:\n",
    "                    new_row.append('0.0')\n",
    "                else:\n",
    "                    try:\n",
    "                        float(item)\n",
    "                        new_row.append(item)\n",
    "                    except:\n",
    "                        new_row.append(float_converter[item])\n",
    "            conversion_labels.append(row[2])\n",
    "            revenue_labels.append(row[3])\n",
    "            X.append(new_row)\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Number of data points: \", len(X))\n",
    "    print(\"Number of features: \", len(X[0]))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf'],\n",
    "                                  'probability': [True]}\n",
    "grid = GridSearchCV(SVC(), parameters, cv=10)\n",
    "grid.fit(X, conversion_labels)\n",
    "params = grid.best_params_\n",
    "print(params)\n",
    "clf = SVC()\n",
    "clf.fit(X, conversion_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "\n",
    "clf2 = lr()\n",
    "clf2.fit(X, revenue_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30375\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-853b087e1aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \"\"\"\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[1;32m    584\u001b[0m                                  \" probability=False\")\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'c_svc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nu_svc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "with open('df_holdout_scholarjet.csv') as csv_file:\n",
    "    my_reader = csv.reader(csv_file)\n",
    "    count = 0\n",
    "    X = []\n",
    "    revenue_labels = []\n",
    "    conversion_labels = []\n",
    "    float_converter = {'Onboarding': '0.0', 'Retention': '1.0', 'Unmanaged': '2.0', 'Enrolled': '0.0', 'Active': '1.0', 'In Progress': '2.0', 'Inactive': '3.0', 'Unconfirmed': '4.0', 'Business': '0.0', 'Trade': '1.0', 'US': '0.0', 'CA': '1.0', 'Internal Application': '0.0', 'Search - Paid': '1.0', 'Internal Customer Scrape': '2.0', 'Affiliates': '3.0', 'External Application': '4.0', 'Social - Paid': '5.0', 'Other': '6.0', 'Email': '7.0', 'Display - Retargeting': '8.0', 'Gateway': '9.0', 'Display - Acquisition': '10.0', 'Bulk Upload': '11.0', 'Quotes': '12.0', 'Referral': '13.0', 'Self ID': '14.0', 'Partners': '15.0', 'None': '0.0', 'Primary': '1.0', 'Purchaser': '2.0', 'Other': '3.0', '6to10': '2.0', '11to50': '3.0', '50plus': '4.0', '2to5': '5.0', '1to2': '1.0', '3to5': '3.0', '11to25': '4.0', '25plus': '5.0', 'lessthan1': '1.0', '1to5': '2.0', '5to25': '3.0', '100plus': '4.0', '25to100': '5.0', 'directEIN': '0.0', 'phone': '1.0', 'email': '2.0', 'liveTransfer': '3.0', 'other': '4.0', 'directOther': '5.0'}\n",
    "    for row in my_reader:\n",
    "        if count > 0:\n",
    "            new_row = []\n",
    "            for item in row[3:]:\n",
    "                if len(item) == 0:\n",
    "                    new_row.append('0.0')\n",
    "                else:\n",
    "                    try:\n",
    "                        float(item)\n",
    "                        new_row.append(item)\n",
    "                    except:\n",
    "                        new_row.append(float_converter[item])\n",
    "            X.append(new_row)\n",
    "        count += 1\n",
    "    \n",
    "    print(len(X))\n",
    "    \n",
    "probabilities = clf.predict_proba(X)\n",
    "print(probabilities)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fig1, ax1 = plt.subplots()\n",
    "# ax1.set_title('Basic Plot')\n",
    "# ax1.boxplot(data)\n",
    "\n",
    "plt.boxplot(df[\"price\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "try:\n",
    "    #Sequential is the classical option for creating a NN in Keras (multiple layers interconnected)\n",
    "    classifier = Sequential()\n",
    "    \"\"\"\n",
    "    We have 101 input features and one target variable (friend/foe), 2 hidden layers each hidden \n",
    "    layer have 4 nodes.\n",
    "    \"\"\"\n",
    "    #First Hidden Layer\n",
    "    classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=191))\n",
    "    #Second  Hidden Layer\n",
    "    classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
    "    #Output Layer\n",
    "    classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "\n",
    "    \"\"\"\n",
    "    *As this is a binary classification problem, we use binary_crossentropy to calculate the loss function \n",
    "    between the actual output and the predicted output.\n",
    "\n",
    "    *To optimize the Neural Network Adam approach is used. Adam stands for Adaptive moment estimation. Adam \n",
    "    is a combination of RMSProp + Momentum. Momentum takes the past gradients into account in order to \n",
    "    smooth out the gradient descent.\n",
    "    \"\"\"\n",
    "    #Compiling the neural network\n",
    "    classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "except IOError as e:\n",
    "    print (\"Could not read file\")\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print (\"Unexpected error\")\n",
    "    print(e)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
